## @section Global parameters
## Global Docker image parameters
## Please, note that this will override the image parameters, including dependencies, configured to use the global value
## Current available global Docker image parameters: imageRegistry, imagePullSecrets and storageClass

## @param global.imageRegistry Global Docker image registry
## @param global.imagePullSecrets Global Docker registry secret names as an array
## @param global.storageClass Global StorageClass for Persistent Volume(s)
##
global:
  imageRegistry: ""
  ## E.g.
  ## imagePullSecrets:
  ##   - myRegistryKeySecretName
  ##
  imagePullSecrets: []
  storageClass: ""

## @section Common parameters

## @param nameOverride String to partially override common.names.fullname template (will maintain the release name)
##
nameOverride: ""
## @param fullnameOverride String to fully override common.names.fullname template
##
fullnameOverride: ""
## @param clusterDomain Kubernetes cluster domain
##
clusterDomain: cluster.local
## @param commonLabels Labels to add to all deployed objects
##
commonLabels: {}
## @param commonAnnotations Annotations to add to all deployed objects
##
commonAnnotations: {}
## @param extraDeploy Array of extra objects to deploy with the release
##
extraDeploy: []

## Enable diagnostic mode in the deployment
##
diagnosticMode:
  ## @param diagnosticMode.enabled Enable diagnostic mode (all probes will be disabled and the command will be overridden)
  ##
  enabled: false
  ## @param diagnosticMode.command Command to override all containers in the deployment
  ##
  command:
    - sleep
  ## @param diagnosticMode.args Args to override all containers in the deployment
  ##
  args:
    - infinity

## @section Opensearch parameters

## Opensearch image version
## ref: https://hub.docker.com/r/opensearchproject/opensearch/tags
## @param image.registry Opensearch image registry
## @param image.repository Opensearch image repository
## @param image.tag Opensearch image tag (immutable tags are recommended)
## @param image.pullPolicy Opensearch image pull policy
## @param image.pullSecrets Opensearch image pull secrets
## @param image.debug Enable image debug mode
##
image:
  registry: docker.io
  repository: opensearchproject/opensearch
  tag: "2.19.2"
  ## Specify a imagePullPolicy
  ## Defaults to 'Always' if image tag is 'latest', else set to 'IfNotPresent'
  ## ref: https://kubernetes.io/docs/user-guide/images/#pre-pulling-images
  ##
  pullPolicy: IfNotPresent
  ## Optionally specify an array of imagePullSecrets.
  ## Secrets must be manually created in the namespace.
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
  ## e.g:
  ## pullSecrets:
  ##   - myRegistryKeySecretName
  ##
  pullSecrets: []
  ## Set to true if you would like to see extra information on logs
  ##
  debug: false

## Security plugin parameters
##
security:
  ## @param security.opensearchPassword Password for 'admin' user
  ##
  opensearchPassword: ""
  ## @param security.dashboardsPassword Password for reserved 'dashboard' user
  ##
  dashboardsPassword: ""
  ## @param security.monitoringPassword Password for reserved 'monitoring' user
  ##
  monitoringPassword: ""
  ## @param security.existingSecret Name of the existing secret containing the Opensearch passwords
  ##
  existingSecret: ""

  ## TLS configuration
  ##
  tls:
    http:
      ## @param security.tls.http.autoGenerated Create cert-manager signed TLS certificates.
      ## Note: Currently only supports PEM with PKCS8 certificates.
      ##
      autoGenerated: true
      ## @param security.tls.http.existingRootCASecret Existing secret containing the tls.key and tls.crt of the Root CA which will sign HTTP certs. This will be used in the cert-manager Issuer.
      ##
      existingRootCASecret: "root-ca-tls"

      ## @param security.tls.http.algorithm Algorithm of the private key. Allowed values are either RSA,Ed25519 or ECDSA.
      algorithm: RSA
      ## @param security.tls.http.size Size is the key bit size of the corresponding private key for this certificate. If algorithm is set to RSA, valid values are 2048, 4096 or 8192, and will default to 2048 if not specified. If algorithm is set to ECDSA, valid values are 256, 384 or 521, and will default to 256 if not specified. If algorithm is set to Ed25519, Size is ignored. No other values are allowed. 
      size: 2048

      ## @param security.tls.http.cluster_manager.existingSecret Existing secret containing the certificates for the cluster_manager nodes
      ## @param security.tls.http.data.existingSecret Existing secret containing the certificates for the data nodes
      ## @param security.tls.http.ingest.existingSecret Existing secret containing the certificates for the ingest nodes
      ## @param security.tls.http.coordinating.existingSecret Existing secret containing the certificates for the coordinating nodes
      ##
      cluster_manager:
        existingSecret: ""
      data:
        existingSecret: ""
      ingest:
        existingSecret: ""
      coordinating:
        existingSecret: ""
      ## @param security.tls.http.keyPassword Password to access the PEM key when they are password-protected.
      ##
      keyPassword: ""
      ## Base subject details for certificate generation, node_dn filtering, admin_dn 
      ##
      subject:
        ## @param security.tls.http.subject.organizations Subject's organization
        ##
        organizations: "example"
        ## @param security.tls.http.subject.countries Subject's country
        ##
        countries: "com"
      issuerRef:
        ## @param security.tls.http.issuerRef.existingIssuerName Existing name of the cert-manager http issuer. If provided, it won't create a default one.
        ##
        existingIssuerName: ""
        ## @param security.tls.http.issuerRef.kind Kind of the cert-manager issuer resource (defaults to "Issuer")
        ##
        kind: "Issuer"
        ## @param security.tls.http.issuerRef.group Group of the cert-manager issuer resource (defaults to "cert-manager.io")
        ##
        group: "cert-manager.io"
    transport:
      ## @param security.tls.transport.enforceHostnameVerification Whether to verify hostnames on the transport layer.
      ## Ref: https://opensearch.org/docs/latest/security/configuration/tls#advanced-hostname-verification-and-dns-lookup
      ##
      enforceHostnameVerification: false
      ## @param security.tls.transport.resolveHostname Whether to resolve hostnames against DNS on the transport layer. Optional. Default is true. Only works if hostname verification is also enabled.
      ## Ref: https://opensearch.org/docs/latest/security/configuration/tls#advanced-hostname-verification-and-dns-lookup
      ##
      resolveHostname: true

      ## @param security.tls.transport.autoGenerated Create cert-manager signed TLS certificates.
      ## Note: Currently only supports PEM with PKCS8 certificates.
      ##
      autoGenerated: true
      ## @param security.tls.transport.existingRootCASecret Existing secret containing the tls.key and tls.crt of the Root CA which will sign transport certs. This will be used in the cert-manager Issuer.
      ##
      existingRootCASecret: "root-ca-tls"

      ## @param security.tls.transport.algorithm Algorithm of the private key. Allowed values are either RSA,Ed25519 or ECDSA.
      algorithm: RSA
      ## @param security.tls.transport.size Size is the key bit size of the corresponding private key for this certificate. If algorithm is set to RSA, valid values are 2048, 4096 or 8192, and will default to 2048 if not specified. If algorithm is set to ECDSA, valid values are 256, 384 or 521, and will default to 256 if not specified. If algorithm is set to Ed25519, Size is ignored. No other values are allowed. 
      size: 2048

      ## @param security.tls.transport.cluster_manager.existingSecret Existing secret containing the certificates for the cluster_manager nodes
      ## @param security.tls.transport.data.existingSecret Existing secret containing the certificates for the data nodes
      ## @param security.tls.transport.ingest.existingSecret Existing secret containing the certificates for the ingest nodes
      ## @param security.tls.transport.coordinating.existingSecret Existing secret containing the certificates for the coordinating nodes
      ##
      cluster_manager:
        existingSecret: ""
      data:
        existingSecret: ""
      ingest:
        existingSecret: ""
      coordinating:
        existingSecret: ""
      ## @param security.tls.transport.keyPassword Password to access the PEM key when they are password-protected.
      ##
      keyPassword: ""
      ## Base subject details for certificate generation, node_dn filtering, admin_dn 
      ##
      subject:
        ## @param security.tls.transport.subject.organizations Subject's organization
        ##
        organizations: "example"
        ## @param security.tls.transport.subject.countries Subject's country
        ##
        countries: "com"
      issuerRef:
        ## @param security.tls.transport.issuerRef.existingIssuerName Existing name of the cert-manager transport issuer. If provided, it won't create a default one.
        ##
        existingIssuerName: ""
        ## @param security.tls.transport.issuerRef.kind Kind of the cert-manager issuer resource (defaults to "Issuer")
        ##
        kind: "Issuer"
        ## @param security.tls.transport.issuerRef.group Group of the cert-manager issuer resource (defaults to "cert-manager.io")
        ##
        group: "cert-manager.io"
    
    truststore:
      ## @param security.tls.truststore.extraCACerts Add extra pem CA certs to the Java truststore
      extraCACerts: {}
        # Values must be YAML literal style scalar / YAML multiline string.
        # <filename>: |
        #   <pem content>
        # root-ca.crt: |
        #   -----BEGIN CERTIFICATE-----
        #   XXXXXXXXXXXXXXXXXXXXXXXXXXX
        #   -----END CERTIFICATE-----


  ## Audit configuration
  ## Ref: https://opensearch.org/docs/latest/security/audit-logs/index/
  ##
  audit:
    ## @param security.audit.type Audit logs let you track access to your OpenSearch cluster and are useful for compliance purposes or in the aftermath of a security breach. You can configure the categories to be logged, the detail level of the logged messages, and where to store the logs. Possible values: <debug|internal_opensearch|external_opensearch|webhook|log4j>
    ## Ref: https://opensearch.org/docs/latest/security/audit-logs/storage-types/
    type: "internal_opensearch"
    ## @param security.audit.index Specify the target index for storage types internal_opensearch or external_opensearch
    ## Ref: https://opensearch.org/docs/latest/security/audit-logs/index/#configure-the-audit-log-index-name
    index: "'security-auditlog-'YYYY.MM.dd"
    ## @param security.audit.ignore_requests You can exclude certain requests from being logged completely, by either configuring actions (for transport requests) and/or HTTP request paths (REST)
    ## Ref: https://opensearch.org/docs/latest/security/audit-logs/index/#exclude-requests
    ignore_requests: []
    ## @param security.audit.ignore_users By default, the security plugin logs events from all users, but excludes the internal OpenSearch Dashboards server users dashboard and monitoring
    ## Ref: https://opensearch.org/docs/latest/security/audit-logs/index/#exclude-users
    ignore_users: []
      #- dashboard
      #- monitoring
    ## @param security.audit.enable_rest By default, the security plugin logs events on the REST layer
    ## Ref: https://opensearch.org/docs/latest/security/audit-logs/index/#disable-rest-or-the-transport-layer
    enable_rest: true
    ## @param security.audit.enable_transport By default, the security plugin logs events on the transport layer
    ## Ref: https://opensearch.org/docs/latest/security/audit-logs/index/#disable-rest-or-the-transport-layer
    enable_transport: true
    ## @param security.audit.resolve_indices By default, the security plugin logs all indices affected by a request. Because index names can be aliases and contain wildcards/date patterns, the security plugin logs the index name that the user submitted and the actual index name to which it resolves.
    ## Ref: https://opensearch.org/docs/latest/security/audit-logs/index/#log-index-names
    resolve_indices: true
    ## @param security.audit.config Configure audit setting
    ## Ref: https://opensearch.org/docs/latest/security/audit-logs/storage-types/
    config: {}

## Opensearch cluster name
## @param clusterName Opensearch cluster name
##
clusterName: opensearch
## @param containerPorts.restAPI Opensearch REST API port
## @param containerPorts.transport Opensearch Transport port
##
containerPorts:
  restAPI: 9200
  transport: 9300
## @param plugins Comma, semi-colon or space separated list of plugins to install at initialization
##
plugins: "repository-s3,https://github.com/aiven/prometheus-exporter-plugin-for-opensearch/releases/download/2.19.2.0/prometheus-exporter-2.19.2.0.zip"

## @param networkHost Network interface to bind (ex: "0.0.0.0", "::" [_local_, _site_])
##
networkHost: "0.0.0.0"

## @param config Override Opensearch configuration
##
config: {}
  # Values must be YAML literal style scalar / YAML multiline string.
  # <filename>: |
  #   <formatted-value(s)>
  # log4j2.properties: |
  #   status = error
  #
  #   appender.console.type = Console
  #   appender.console.name = console
  #   appender.console.layout.type = PatternLayout
  #   appender.console.layout.pattern = [%d{ISO8601}][%-5p][%-25c{1.}] [%node_name]%marker %m%n
  #
  #   rootLogger.level = info
  #   rootLogger.appenderRef.console.ref = console

  # log4j2.properties:

## Opensearch shard allocation awareness or forced awareness
## @param allocationAwareness.enabled Enable allocation awareness
## @param allocationAwareness.topologyKey Node label used for topologyKey
## @param allocationAwareness.forceZones.enabled Require that primary and replica shards are never allocated to the same zone
## @param allocationAwareness.forceZones.zones To configure forced awareness, specify all the possible values for your zone attributes
## ref: https://opensearch.org/docs/latest/opensearch/cluster/#advanced-step-6-configure-shard-allocation-awareness-or-forced-awareness
## 
allocationAwareness:
  enabled: false
  topologyKey: "topology.kubernetes.io/zone"
  forceZones:
    enabled: false
    zones: []
      #- "zone1"
      #- "zone2"
      #- "zone3"

## @param extraConfig Append extra configuration to the Opensearch node configuration
## Use this instead of `config` to add more configuration
## See below example:
## extraConfig:
##   node:
##     store:
##       allow_mmap: false
extraConfig: {}
## @param extraVolumes A list of volumes to be added to the pod
## Example Use Case: mount ssl certificates when Opensearch has tls enabled
## extraVolumes:
##   - name: os-certs
##     secret:
##       defaultMode: 420
##       secretName: os-certs
extraVolumes: []
## @param extraVolumeMounts A list of volume mounts to be added to the pod
## extraVolumeMounts:
##   - name: os-certs
##     mountPath: /certs
##     readOnly: true
extraVolumeMounts: []
## @param extraEnvVars Array containing extra env vars to be added to all pods (evaluated as a template)
## For example:
## extraEnvVars:
##  - name: MY_ENV_VAR
##    value: env_var_value
##
extraEnvVars: []
## @param extraEnvVarsConfigMap ConfigMap containing extra env vars to be added to all pods (evaluated as a template)
##
extraEnvVarsConfigMap: ""
## @param extraEnvVarsSecret Secret containing extra env vars to be added to all pods (evaluated as a template)
##
extraEnvVarsSecret: ""

## @section Master parameters

## Opensearch cluster_manager node parameters
##
cluster_manager:
  ## @param cluster_manager.fullnameOverride String to fully override opensearch.cluster_manager.fullname template with a string
  ##
  fullnameOverride: ""
  ## @param cluster_manager.replicaCount Desired number of Opensearch cluster_manager nodes. Consider using an odd number of cluster_manager nodes to prevent "split brain" situation.  See: https://opensearch.org/docs/latest/opensearch/cluster/
  ##
  replicaCount: 3
  ## Update strategy for Opensearch cluster_manager statefulset
  ## ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#update-strategies
  ## @param cluster_manager.updateStrategy.type Update strategy for Master statefulset
  ##
  updateStrategy:
    type: RollingUpdate
  ## @param cluster_manager.hostAliases Add deployment host aliases
  ## https://kubernetes.io/docs/concepts/services-networking/add-entries-to-pod-etc-hosts-with-host-aliases/
  ##
  hostAliases: []
  ## @param cluster_manager.schedulerName Name of the k8s scheduler (other than default)
  ## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/
  ##
  schedulerName: ""
  ##
  ## @param cluster_manager.heapSize Master-eligible node heap size
  ##
  heapSize: 128m
  ## @param cluster_manager.podAnnotations Annotations for cluster_manager pods.
  ##
  podAnnotations: {}
  ## @param cluster_manager.podLabels Extra labels to add to Pod
  ##
  podLabels: {}
  ## Pod Security Context for cluster_manager pods.
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  ## @param cluster_manager.securityContext.enabled Enable security context for cluster_manager pods
  ## @param cluster_manager.securityContext.fsGroup Group ID for the container for cluster_manager pods
  ## @param cluster_manager.securityContext.runAsUser User ID for the container for cluster_manager pods
  ##
  securityContext:
    enabled: true
    fsGroup: 1000
    runAsUser: 1000
  ## Pod Security Context for cluster_manager pods.
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  ## @param cluster_manager.podSecurityContext.enabled Enable security context for cluster_manager pods
  ## @param cluster_manager.podSecurityContext.fsGroup Group ID for the container for cluster_manager pods
  ##
  podSecurityContext:
    enabled: false
    fsGroup: 1000
  ## Container Security Context for the main container.
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  ## @param cluster_manager.containerSecurityContext.enabled Enable security context for the main container
  ## @param cluster_manager.containerSecurityContext.runAsUser User ID for the container for the main container
  ## @param cluster_manager.containerSecurityContext.runAsNonRoot Indicates that the container must run as a non-root user
  ## @param cluster_manager.containerSecurityContext.allowPrivilegeEscalation Controls whether a process can gain more privileges than its parent process
  ##
  containerSecurityContext:
    enabled: true
    runAsUser: 1000
    runAsNonRoot: true
    allowPrivilegeEscalation: false

  ## @param cluster_manager.podAffinityPreset Master-eligible Pod affinity preset. Ignored if `affinity` is set. Allowed values: `soft` or `hard`
  ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
  ##
  podAffinityPreset: ""
  ## @param cluster_manager.podAntiAffinityPreset Master-eligible Pod anti-affinity preset. Ignored if `affinity` is set. Allowed values: `soft` or `hard`
  ## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
  ##
  podAntiAffinityPreset: ""
  ## Node affinity preset. Allowed values: soft, hard
  ## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity
  ## @param cluster_manager.nodeAffinityPreset.type Master-eligible Node affinity preset type. Ignored if `affinity` is set. Allowed values: `soft` or `hard`
  ## @param cluster_manager.nodeAffinityPreset.key Master-eligible Node label key to match Ignored if `affinity` is set.
  ## @param cluster_manager.nodeAffinityPreset.values Master-eligible Node label values to match. Ignored if `affinity` is set.
  ##
  nodeAffinityPreset:
    type: ""
    ## E.g.
    ## key: "kubernetes.io/e2e-az-name"
    ##
    key: ""
    ## E.g.
    ## values:
    ##   - e2e-az1
    ##   - e2e-az2
    ##
    values: []
  ## @param cluster_manager.affinity Master-eligible Affinity for pod assignment
  ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
  ## Note: podAffinityPreset, podAntiAffinityPreset, and  nodeAffinityPreset will be ignored when it's set
  ##
  affinity: {}
  ## @param cluster_manager.priorityClassName Master pods Priority Class Name
  ## ref: https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/#priorityclass
  ##
  priorityClassName: ""
  ## @param cluster_manager.nodeSelector Master-eligible Node labels for pod assignment
  ## Ref: https://kubernetes.io/docs/user-guide/node-selection/
  ##
  nodeSelector: {}
  ## @param cluster_manager.tolerations Master-eligible Tolerations for pod assignment
  ## Ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
  ##
  tolerations: []
  ## @param cluster_manager.topologySpreadConstraints Topology Spread Constraints for pod assignment spread across your cluster among failure-domains. Evaluated as a template
  ## Ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/#spread-constraints-for-pods
  ##
  topologySpreadConstraints: []
  ## Opensearch cluster_manager container's resource requests and limits
  ## ref: https://kubernetes.io/docs/user-guide/compute-resources/
  ## We usually recommend not to specify default resources and to leave this as a conscious
  ## choice for the user. This also increases chances charts run on environments with little
  ## resources, such as Minikube. If you do want to specify resources, uncomment the following
  ## lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  ## @param cluster_manager.resources.limits The resources limits for the container
  ## @param cluster_manager.resources.requests [object] The requested resources for the container
  ##
  resources:
    ## Example:
    ## limits:
    ##    cpu: 100m
    ##    memory: 128Mi
    limits: {}
    requests:
      cpu: 25m
      memory: 256Mi
  ## Opensearch cluster_manager container's startup probe
  ## ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#container-probes
  ## @param cluster_manager.startupProbe.enabled Enable/disable the startup probe (cluster_manager nodes pod)
  ## @param cluster_manager.startupProbe.initialDelaySeconds Delay before startup probe is initiated (cluster_manager nodes pod)
  ## @param cluster_manager.startupProbe.periodSeconds How often to perform the probe (cluster_manager nodes pod)
  ## @param cluster_manager.startupProbe.timeoutSeconds When the probe times out (cluster_manager nodes pod)
  ## @param cluster_manager.startupProbe.successThreshold Minimum consecutive successes for the probe to be considered successful after having failed (cluster_manager nodes pod)
  ## @param cluster_manager.startupProbe.failureThreshold Minimum consecutive failures for the probe to be considered failed after having succeeded
  ##
  startupProbe:
    enabled: false
    initialDelaySeconds: 90
    periodSeconds: 10
    timeoutSeconds: 5
    successThreshold: 1
    failureThreshold: 5
  ## Opensearch cluster_manager container's liveness probe
  ## ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#container-probes
  ## @param cluster_manager.livenessProbe.enabled Enable/disable the liveness probe (cluster_manager nodes pod)
  ## @param cluster_manager.livenessProbe.initialDelaySeconds Delay before liveness probe is initiated (cluster_manager nodes pod)
  ## @param cluster_manager.livenessProbe.periodSeconds How often to perform the probe (cluster_manager nodes pod)
  ## @param cluster_manager.livenessProbe.timeoutSeconds When the probe times out (cluster_manager nodes pod)
  ## @param cluster_manager.livenessProbe.successThreshold Minimum consecutive successes for the probe to be considered successful after having failed (cluster_manager nodes pod)
  ## @param cluster_manager.livenessProbe.failureThreshold Minimum consecutive failures for the probe to be considered failed after having succeeded
  ##
  livenessProbe:
    enabled: false
    initialDelaySeconds: 90
    periodSeconds: 10
    timeoutSeconds: 5
    successThreshold: 1
    failureThreshold: 5
  ## Opensearch cluster_manager container's readiness probe
  ## ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#container-probes
  ## @param cluster_manager.readinessProbe.enabled Enable/disable the readiness probe (cluster_manager nodes pod)
  ## @param cluster_manager.readinessProbe.initialDelaySeconds Delay before readiness probe is initiated (cluster_manager nodes pod)
  ## @param cluster_manager.readinessProbe.periodSeconds How often to perform the probe (cluster_manager nodes pod)
  ## @param cluster_manager.readinessProbe.timeoutSeconds When the probe times out (cluster_manager nodes pod)
  ## @param cluster_manager.readinessProbe.successThreshold Minimum consecutive successes for the probe to be considered successful after having failed (cluster_manager nodes pod)
  ## @param cluster_manager.readinessProbe.failureThreshold Minimum consecutive failures for the probe to be considered failed after having succeeded
  ##
  readinessProbe:
    enabled: true
    initialDelaySeconds: 30
    periodSeconds: 10
    timeoutSeconds: 5
    successThreshold: 1
    failureThreshold: 5
  ## @param cluster_manager.customStartupProbe Override default startup probe
  ##
  customStartupProbe: {}
  ## @param cluster_manager.customLivenessProbe Override default liveness probe
  ##
  customLivenessProbe: {}
  ## @param cluster_manager.customReadinessProbe Override default readiness probe
  ##
  customReadinessProbe: {}
  ## @param cluster_manager.initContainers Extra init containers to add to the Opensearch cluster_manager pod(s)
  ##
  initContainers: []
  ## @param cluster_manager.sidecars Extra sidecar containers to add to the Opensearch cluster_manager pod(s)
  ##
  sidecars: []
  ## Enable persistence using Persistent Volume Claims
  ## ref: https://kubernetes.io/docs/user-guide/persistent-volumes/
  ##
  persistence:
    ## @param cluster_manager.persistence.enabled Enable persistence using a `PersistentVolumeClaim`
    ##
    enabled: true
    ## @param cluster_manager.persistence.storageClass Persistent Volume Storage Class
    ## If defined, storageClassName: <storageClass>
    ## If set to "-", storageClassName: "", which disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is
    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
    ##   GKE, AWS & OpenStack)
    ##
    storageClass: ""
    ## @param cluster_manager.persistence.existingClaim Existing Persistent Volume Claim
    ## then accept the value as an existing Persistent Volume Claim to which
    ## the container should be bound
    ##
    existingClaim: ""
    ## @param cluster_manager.persistence.existingVolume Existing Persistent Volume for use as volume match label selector to the `volumeClaimTemplate`. Ignored when `cluster_manager.persistence.selector` is set.
    ##
    existingVolume: ""
    ## @param cluster_manager.persistence.selector Configure custom selector for existing Persistent Volume. Overwrites `cluster_manager.persistence.existingVolume`
    ## selector:
    ##   matchLabels:
    ##     volume:
    ##
    selector: {}
    ## @param cluster_manager.persistence.annotations Persistent Volume Claim annotations
    ##
    annotations: {}
    ## @param cluster_manager.persistence.accessModes Persistent Volume Access Modes
    ##
    accessModes:
      - ReadWriteOnce
    ## @param cluster_manager.persistence.size Persistent Volume Size
    ##
    size: 8Gi
  ## Service parameters for cluster_manager node(s)
  ##
  service:
    ## @param cluster_manager.service.type Kubernetes Service type (cluster_manager nodes)
    ##
    type: ClusterIP
    ## @param cluster_manager.service.ports.restAPI Opensearch service REST API port
    ## @param cluster_manager.service.ports.transport Opensearch service transport port
    ##
    ports:
      restAPI: 9200
      transport: 9300
    ## @param cluster_manager.service.nodePort Kubernetes Service nodePort (cluster_manager nodes)
    ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#type-nodeport
    ##
    nodePort: ""
    ## @param cluster_manager.service.annotations Annotations for cluster_manager nodes service
    ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#internal-load-balancer
    ##
    annotations: {}
    ## @param cluster_manager.service.loadBalancerIP loadBalancerIP if cluster_manager nodes service type is `LoadBalancer`
    ## Set the LoadBalancer service type to internal only.
    ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#internal-load-balancer
    ##
    loadBalancerIP: ""
    ## @param cluster_manager.service.ipFamilyPolicy ipFamilyPolicy for cluster_manager nodes service
    ## ref: https://kubernetes.io/docs/concepts/services-networking/dual-stack/#services
    ##
    ipFamilyPolicy: PreferDualStack

  ## Pods Service Account
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  ## @param cluster_manager.serviceAccount.create Specifies whether a ServiceAccount should be created
  ## @param cluster_manager.serviceAccount.name Name of the service account to use. If not set and create is true, a name is generated using the fullname template.
  ## @param cluster_manager.serviceAccount.automountServiceAccountToken Automount service account token for the server service account
  ## @param cluster_manager.serviceAccount.annotations Annotations for service account. Evaluated as a template. Only used if `create` is `true`.
  ##
  serviceAccount:
    create: true
    name: ""
    automountServiceAccountToken: false
    annotations: {}

  ## Enable HorizontalPodAutoscaler for Opensearch Master pods
  ## ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/
  ## @param cluster_manager.autoscaling.enabled Whether enable horizontal pod autoscale
  ## @param cluster_manager.autoscaling.minReplicas Configure a minimum amount of pods
  ## @param cluster_manager.autoscaling.maxReplicas Configure a maximum amount of pods
  ## @param cluster_manager.autoscaling.targetCPU Define the CPU target to trigger the scaling actions (utilization percentage)
  ## @param cluster_manager.autoscaling.targetMemory Define the memory target to trigger the scaling actions (utilization percentage)
  ##
  autoscaling:
    enabled: false
    minReplicas: 3
    maxReplicas: 11
    targetCPU: ""
    targetMemory: ""

## @section Coordinating parameters

## Opensearch coordinating node parameters
##
coordinating:
  ## @param coordinating.fullnameOverride String to fully override opensearch.coordinating.fullname template with a string
  ##
  fullnameOverride: ""
  ## @param coordinating.replicaCount Desired number of Opensearch coordinating nodes
  ##
  replicaCount: 2
  ## @param coordinating.hostAliases Add deployment host aliases
  ## https://kubernetes.io/docs/concepts/services-networking/add-entries-to-pod-etc-hosts-with-host-aliases/
  ##
  hostAliases: []
  ## @param coordinating.schedulerName Name of the k8s scheduler (other than default)
  ## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/
  ##
  schedulerName: ""
  ## Update strategy for Opensearch coordinating statefulset
  ## ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#update-strategies
  ## @param coordinating.updateStrategy.type Update strategy for Coordinating Statefulset
  ##
  updateStrategy:
    type: RollingUpdate
  ## @param coordinating.heapSize coordinating node heap size
  ##
  heapSize: 128m
  ## @param coordinating.podAnnotations Annotations for coordinating pods.
  ##
  podAnnotations: {}
  ## @param coordinating.podLabels Extra labels to add to Pod
  ##
  podLabels: {}
  ## Pod Security Context for coordinating pods.
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  ## @param coordinating.securityContext.enabled Enable security context for coordinating pods
  ## @param coordinating.securityContext.fsGroup Group ID for the container for coordinating pods
  ## @param coordinating.securityContext.runAsUser User ID for the container for coordinating pods
  ##
  securityContext:
    enabled: true
    fsGroup: 1000
    runAsUser: 1000
  ## Pod Security Context for coordinating pods.
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  ## @param coordinating.podSecurityContext.enabled Enable security context for coordinating pods
  ## @param coordinating.podSecurityContext.fsGroup Group ID for the container for coordinating pods
  ##
  podSecurityContext:
    enabled: false
    fsGroup: 1000
  ## Container Security Context for the main container.
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  ## @param coordinating.containerSecurityContext.enabled Enable security context for the main container
  ## @param coordinating.containerSecurityContext.runAsUser User ID for the container for the main container
  ## @param coordinating.containerSecurityContext.runAsNonRoot Indicates that the container must run as a non-root user
  ## @param coordinating.containerSecurityContext.allowPrivilegeEscalation Controls whether a process can gain more privileges than its parent process
  ##
  containerSecurityContext:
    enabled: true
    runAsUser: 1000
    runAsNonRoot: true
    allowPrivilegeEscalation: false
  ## @param coordinating.podAffinityPreset Coordinating Pod affinity preset. Ignored if `affinity` is set. Allowed values: `soft` or `hard`
  ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
  ##
  podAffinityPreset: ""
  ## @param coordinating.podAntiAffinityPreset Coordinating Pod anti-affinity preset. Ignored if `affinity` is set. Allowed values: `soft` or `hard`
  ## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
  ##
  podAntiAffinityPreset: ""
  ## Node affinity preset
  ## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity
  ## @param coordinating.nodeAffinityPreset.type Coordinating Node affinity preset type. Ignored if `affinity` is set. Allowed values: `soft` or `hard`
  ## @param coordinating.nodeAffinityPreset.key Coordinating Node label key to match Ignored if `affinity` is set.
  ## @param coordinating.nodeAffinityPreset.values Coordinating Node label values to match. Ignored if `affinity` is set.
  ##
  nodeAffinityPreset:
    type: ""
    ## E.g.
    ## key: "kubernetes.io/e2e-az-name"
    ##
    key: ""
    ## E.g.
    ## values:
    ##   - e2e-az1
    ##   - e2e-az2
    ##
    values: []
  ## @param coordinating.affinity Coordinating Affinity for pod assignment
  ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
  ## Note: podAffinityPreset, podAntiAffinityPreset, and  nodeAffinityPreset will be ignored when it's set
  ##
  affinity: {}
  ## @param coordinating.priorityClassName Coordinating pods Priority Class Name
  ## ref: https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/#priorityclass
  ##
  priorityClassName: ""
  ## @param coordinating.nodeSelector Coordinating Node labels for pod assignment
  ## Ref: https://kubernetes.io/docs/user-guide/node-selection/
  ##
  nodeSelector: {}
  ## @param coordinating.tolerations Coordinating Tolerations for pod assignment
  ## Ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
  ##
  tolerations: []
  ## @param coordinating.topologySpreadConstraints Topology Spread Constraints for pod assignment spread across your cluster among failure-domains. Evaluated as a template
  ## Ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/#spread-constraints-for-pods
  ##
  topologySpreadConstraints: []
  ## Opensearch coordinating container's resource requests and limits
  ## ref: https://kubernetes.io/docs/user-guide/compute-resources/
  ## We usually recommend not to specify default resources and to leave this as a conscious
  ## choice for the user. This also increases chances charts run on environments with little
  ## resources, such as Minikube. If you do want to specify resources, uncomment the following
  ## lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  ## @param coordinating.resources.limits The resources limits for the container
  ## @param coordinating.resources.requests [object] The requested resources for the container
  ##
  resources:
    ## Example:
    ## limits:
    ##    cpu: 100m
    ##    memory: 384Mi
    limits: {}
    requests:
      cpu: 25m
      memory: 256Mi
  ## Opensearch coordinating container's startup probe
  ## ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#container-probes
  ## @param coordinating.startupProbe.enabled Enable/disable the startup probe (coordinating nodes pod)
  ## @param coordinating.startupProbe.initialDelaySeconds Delay before startup probe is initiated (coordinating nodes pod)
  ## @param coordinating.startupProbe.periodSeconds How often to perform the probe (coordinating nodes pod)
  ## @param coordinating.startupProbe.timeoutSeconds When the probe times out (coordinating nodes pod)
  ## @param coordinating.startupProbe.failureThreshold Minimum consecutive failures for the probe to be considered failed after having succeeded
  ## @param coordinating.startupProbe.successThreshold Minimum consecutive successes for the probe to be considered successful after having failed (coordinating nodes pod)
  ##
  startupProbe:
    enabled: false
    initialDelaySeconds: 90
    periodSeconds: 10
    timeoutSeconds: 5
    successThreshold: 1
    failureThreshold: 5
  ## Opensearch coordinating container's liveness probe
  ## ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#container-probes
  ## @param coordinating.livenessProbe.enabled Enable/disable the liveness probe (coordinating nodes pod)
  ## @param coordinating.livenessProbe.initialDelaySeconds Delay before liveness probe is initiated (coordinating nodes pod)
  ## @param coordinating.livenessProbe.periodSeconds How often to perform the probe (coordinating nodes pod)
  ## @param coordinating.livenessProbe.timeoutSeconds When the probe times out (coordinating nodes pod)
  ## @param coordinating.livenessProbe.failureThreshold Minimum consecutive failures for the probe to be considered failed after having succeeded
  ## @param coordinating.livenessProbe.successThreshold Minimum consecutive successes for the probe to be considered successful after having failed (coordinating nodes pod)
  ##
  livenessProbe:
    enabled: false
    initialDelaySeconds: 90
    periodSeconds: 10
    timeoutSeconds: 5
    successThreshold: 1
    failureThreshold: 5
  ## Opensearch coordinating container's readiness probe
  ## ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#container-probes
  ## @param coordinating.readinessProbe.enabled Enable/disable the readiness probe (coordinating nodes pod)
  ## @param coordinating.readinessProbe.initialDelaySeconds Delay before readiness probe is initiated (coordinating nodes pod)
  ## @param coordinating.readinessProbe.periodSeconds How often to perform the probe (coordinating nodes pod)
  ## @param coordinating.readinessProbe.timeoutSeconds When the probe times out (coordinating nodes pod)
  ## @param coordinating.readinessProbe.failureThreshold Minimum consecutive failures for the probe to be considered failed after having succeeded
  ## @param coordinating.readinessProbe.successThreshold Minimum consecutive successes for the probe to be considered successful after having failed (coordinating nodes pod)
  ##
  readinessProbe:
    enabled: true
    initialDelaySeconds: 30
    periodSeconds: 10
    timeoutSeconds: 5
    successThreshold: 1
    failureThreshold: 5
  ## @param coordinating.customStartupProbe Override default startup probe
  ##
  customStartupProbe: {}
  ## @param coordinating.customLivenessProbe Override default liveness probe
  ##
  customLivenessProbe: {}
  ## @param coordinating.customReadinessProbe Override default readiness probe
  ##
  customReadinessProbe: {}
  ## @param coordinating.initContainers Extra init containers to add to the Opensearch coordinating pod(s)
  ##
  initContainers: []
  ## @param coordinating.sidecars Extra sidecar containers to add to the Opensearch coordinating pod(s)
  ##
  sidecars: []
  ## Service parameters for coordinating node(s)
  ##
  service:
    ## @param coordinating.service.type Kubernetes Service type (coordinating nodes)
    ##
    type: ClusterIP
    ## @param coordinating.service.ports.restAPI Opensearch service REST API port
    ## @param coordinating.service.ports.transport Opensearch service transport port
    ##
    ports:
      restAPI: 9200
      transport: 9300
    ## @param coordinating.service.nodePort Kubernetes Service nodePort (coordinating nodes)
    ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#type-nodeport
    ##
    nodePort: ""
    ## @param coordinating.service.annotations Annotations for coordinating nodes service
    ## Set the LoadBalancer service type to internal only.
    ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#internal-load-balancer
    ##
    annotations: {}
    ## @param coordinating.service.loadBalancerIP loadBalancerIP if coordinating nodes service type is `LoadBalancer`
    ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#internal-load-balancer
    ##
    loadBalancerIP: ""
    ## @param coordinating.service.externalTrafficPolicy Enable client source IP preservation with externalTrafficPolicy: Local
    ## ref http://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip
    ##
    externalTrafficPolicy: Cluster
    ## @param coordinating.service.ipFamilyPolicy ipFamilyPolicy for coordinating nodes service
    ## ref: https://kubernetes.io/docs/concepts/services-networking/dual-stack/#services
    ##
    ipFamilyPolicy: PreferDualStack
  ## Pods Service Account
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  ## @param coordinating.serviceAccount.create Specifies whether a ServiceAccount should be created
  ## @param coordinating.serviceAccount.name Name of the service account to use. If not set and create is true, a name is generated using the fullname template.
  ## @param coordinating.serviceAccount.automountServiceAccountToken Automount service account token for the server service account
  ## @param coordinating.serviceAccount.annotations Annotations for service account. Evaluated as a template. Only used if `create` is `true`.
  ##
  serviceAccount:
    create: true
    name: ""
    automountServiceAccountToken: false
    annotations: {}
  ## Enable HorizontalPodAutoscaler for Opensearch coordinating pods
  ## ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/
  ## @param coordinating.autoscaling.enabled Whether enable horizontal pod autoscale
  ## @param coordinating.autoscaling.minReplicas Configure a minimum amount of pods
  ## @param coordinating.autoscaling.maxReplicas Configure a maximum amount of pods
  ## @param coordinating.autoscaling.targetCPU Define the CPU target to trigger the scaling actions (utilization percentage)
  ## @param coordinating.autoscaling.targetMemory Define the memory target to trigger the scaling actions (utilization percentage)
  ##
  autoscaling:
    enabled: false
    minReplicas: 3
    maxReplicas: 11
    targetCPU: ""
    targetMemory: ""

## @section Data parameters

## Opensearch data node parameters
##
data:
  ## @param data.fullnameOverride String to fully override opensearch.data.fullname template with a string
  ##
  fullnameOverride: ""
  ## @param data.replicaCount Desired number of Opensearch data nodes
  ##
  replicaCount: 2
  ## @param data.hostAliases Add deployment host aliases
  ## https://kubernetes.io/docs/concepts/services-networking/add-entries-to-pod-etc-hosts-with-host-aliases/
  ##
  hostAliases: []
  ## @param data.schedulerName Name of the k8s scheduler (other than default)
  ## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/
  ##
  schedulerName: ""
  ## Update strategy for Opensearch Data statefulset
  ## ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#update-strategies
  ## @param data.updateStrategy.type Update strategy for Data statefulset
  ## @param data.updateStrategy.rollingUpdatePartition Partition update strategy for Data statefulset
  ##
  updateStrategy:
    type: RollingUpdate
    rollingUpdatePartition: ""
  ## @param data.heapSize Data node heap size
  ##
  heapSize: 1024m
  ## @param data.podAnnotations Annotations for data pods.
  ##
  podAnnotations: {}
  ## @param data.podLabels Extra labels to add to Pod
  ##
  podLabels: {}
  ## Pod Security Context for data pods.
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  ## @param data.securityContext.enabled Enable security context for data pods
  ## @param data.securityContext.fsGroup Group ID for the container for data pods
  ## @param data.securityContext.runAsUser User ID for the container for data pods
  ##
  securityContext:
    enabled: true
    fsGroup: 1000
    runAsUser: 1000
  ## Pod Security Context for data pods.
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  ## @param data.podSecurityContext.enabled Enable security context for data pods
  ## @param data.podSecurityContext.fsGroup Group ID for the container for data pods
  ##
  podSecurityContext:
    enabled: true
    fsGroup: 1000
  ## Container Security Context for the main container.
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  ## @param data.containerSecurityContext.enabled Enable security context for the main container
  ## @param data.containerSecurityContext.runAsUser User ID for the container for the main container
  ## @param data.containerSecurityContext.runAsNonRoot Indicates that the container must run as a non-root user
  ## @param data.containerSecurityContext.allowPrivilegeEscalation Controls whether a process can gain more privileges than its parent process
  ##
  containerSecurityContext:
    enabled: true
    runAsUser: 1000
    runAsNonRoot: true
    allowPrivilegeEscalation: false
  ## @param data.podAffinityPreset Data Pod affinity preset. Ignored if `affinity` is set. Allowed values: `soft` or `hard`
  ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
  ##
  podAffinityPreset: ""
  ## @param data.podAntiAffinityPreset Data Pod anti-affinity preset. Ignored if `affinity` is set. Allowed values: `soft` or `hard`
  ## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
  ##
  podAntiAffinityPreset: ""
  ## Node affinity preset. Allowed values: soft, hard
  ## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity
  ## @param data.nodeAffinityPreset.type Data Node affinity preset type. Ignored if `affinity` is set. Allowed values: `soft` or `hard`
  ## @param data.nodeAffinityPreset.key Data Node label key to match Ignored if `affinity` is set.
  ## @param data.nodeAffinityPreset.values Data Node label values to match. Ignored if `affinity` is set.
  ##
  nodeAffinityPreset:
    type: ""
    ## E.g.
    ## key: "kubernetes.io/e2e-az-name"
    ##
    key: ""
    ## E.g.
    ## values:
    ##   - e2e-az1
    ##   - e2e-az2
    ##
    values: []

  ## @param data.affinity Data Affinity for pod assignment
  ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
  ## Note: podAffinityPreset, podAntiAffinityPreset, and  nodeAffinityPreset will be ignored when it's set
  ##
  affinity: {}
  ## @param data.priorityClassName Data pods Priority Class Name
  ## ref: https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/#priorityclass
  ##
  priorityClassName: ""
  ## @param data.nodeSelector Data Node labels for pod assignment
  ## Ref: https://kubernetes.io/docs/user-guide/node-selection/
  ##
  nodeSelector: {}
  ## @param data.tolerations Data Tolerations for pod assignment
  ## Ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
  ##
  tolerations: []
  ## @param data.topologySpreadConstraints Topology Spread Constraints for pod assignment spread across your cluster among failure-domains. Evaluated as a template
  ## Ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/#spread-constraints-for-pods
  ##
  topologySpreadConstraints: []
  ## Opensearch data container's resource requests and limits
  ## ref: https://kubernetes.io/docs/user-guide/compute-resources/
  ## We usually recommend not to specify default resources and to leave this as a conscious
  ## choice for the user. This also increases chances charts run on environments with little
  ## resources, such as Minikube. If you do want to specify resources, uncomment the following
  ## lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  ## @param data.resources.limits The resources limits for the container
  ## @param data.resources.requests [object] The requested resources for the container
  ##
  resources:
    ## Example:
    ## limits:
    ##    cpu: 100m
    ##    memory: 2176Mi
    limits: {}
    requests:
      cpu: 25m
      memory: 2048Mi
  ## Opensearch data container's startup probe
  ## ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#container-probes
  ## @param data.startupProbe.enabled Enable/disable the startup probe (data nodes pod)
  ## @param data.startupProbe.initialDelaySeconds Delay before startup probe is initiated (data nodes pod)
  ## @param data.startupProbe.periodSeconds How often to perform the probe (data nodes pod)
  ## @param data.startupProbe.timeoutSeconds When the probe times out (data nodes pod)
  ## @param data.startupProbe.failureThreshold Minimum consecutive failures for the probe to be considered failed after having succeeded
  ## @param data.startupProbe.successThreshold Minimum consecutive successes for the probe to be considered successful after having failed (data nodes pod)
  ##
  startupProbe:
    enabled: false
    initialDelaySeconds: 90
    periodSeconds: 10
    timeoutSeconds: 5
    successThreshold: 1
    failureThreshold: 5
  ## Opensearch data container's liveness probe
  ## ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#container-probes
  ## @param data.livenessProbe.enabled Enable/disable the liveness probe (data nodes pod)
  ## @param data.livenessProbe.initialDelaySeconds Delay before liveness probe is initiated (data nodes pod)
  ## @param data.livenessProbe.periodSeconds How often to perform the probe (data nodes pod)
  ## @param data.livenessProbe.timeoutSeconds When the probe times out (data nodes pod)
  ## @param data.livenessProbe.failureThreshold Minimum consecutive failures for the probe to be considered failed after having succeeded
  ## @param data.livenessProbe.successThreshold Minimum consecutive successes for the probe to be considered successful after having failed (data nodes pod)
  ##
  livenessProbe:
    enabled: false
    initialDelaySeconds: 90
    periodSeconds: 10
    timeoutSeconds: 5
    successThreshold: 1
    failureThreshold: 5
  ## Opensearch data container's readiness probe
  ## ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#container-probes
  ## @param data.readinessProbe.enabled Enable/disable the readiness probe (data nodes pod)
  ## @param data.readinessProbe.initialDelaySeconds Delay before readiness probe is initiated (data nodes pod)
  ## @param data.readinessProbe.periodSeconds How often to perform the probe (data nodes pod)
  ## @param data.readinessProbe.timeoutSeconds When the probe times out (data nodes pod)
  ## @param data.readinessProbe.failureThreshold Minimum consecutive failures for the probe to be considered failed after having succeeded
  ## @param data.readinessProbe.successThreshold Minimum consecutive successes for the probe to be considered successful after having failed (data nodes pod)
  ##
  readinessProbe:
    enabled: true
    initialDelaySeconds: 30
    periodSeconds: 10
    timeoutSeconds: 5
    successThreshold: 1
    failureThreshold: 5
  ## @param data.customStartupProbe Override default startup probe
  ##
  customStartupProbe: {}
  ## @param data.customLivenessProbe Override default liveness probe
  ##
  customLivenessProbe: {}
  ## @param data.customReadinessProbe Override default readiness probe
  ##
  customReadinessProbe: {}
  ## @param data.initContainers Extra init containers to add to the Opensearch data pod(s)
  ##
  initContainers: []
  ## @param data.sidecars Extra sidecar containers to add to the Opensearch data pod(s)
  ##
  sidecars: []
  ## Service parameters for data-eligible node(s)
  ##
  service:
    ## @param data.service.annotations Annotations for data-eligible nodes service
    ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#internal-load-balancer
    ##
    annotations: {}
    ## @param data.service.ipFamilyPolicy ipFamilyPolicy for data nodes service
    ## ref: https://kubernetes.io/docs/concepts/services-networking/dual-stack/#services
    ##
    ipFamilyPolicy: PreferDualStack
  ## Enable persistence using Persistent Volume Claims
  ## ref: https://kubernetes.io/docs/user-guide/persistent-volumes/
  ##
  persistence:
    ## @param data.persistence.enabled Enable persistence using a `PersistentVolumeClaim`
    ##
    enabled: true
    ## @param data.persistence.storageClass Persistent Volume Storage Class
    ## If defined, storageClassName: <storageClass>
    ## If set to "-", storageClassName: "", which disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is
    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
    ##   GKE, AWS & OpenStack)
    ##
    storageClass: ""
    ## @param data.persistence.existingClaim Existing Persistent Volume Claim
    ## If persistence is enable, and this value is defined,
    ## then accept the value as an existing Persistent Volume Claim to which
    ## the container should be bound
    ##
    existingClaim: ""
    ## @param data.persistence.existingVolume Existing Persistent Volume for use as volume match label selector to the `volumeClaimTemplate`. Ignored when `data.persistence.selector` ist set.
    ##
    existingVolume: ""
    ## @param data.persistence.selector Configure custom selector for existing Persistent Volume. Overwrites `data.persistence.existingVolume`
    ## selector:
    ##   matchLabels:
    ##     volume:
    selector: {}
    ## @param data.persistence.annotations Persistent Volume Claim annotations
    ##
    annotations: {}
    ## @param data.persistence.accessModes Persistent Volume Access Modes
    ##
    accessModes:
      - ReadWriteOnce
    ## @param data.persistence.size Persistent Volume Size
    ##
    size: 8Gi
  ## Pods Service Account
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  ## @param data.serviceAccount.create Specifies whether a ServiceAccount should be created
  ## @param data.serviceAccount.name Name of the service account to use. If not set and create is true, a name is generated using the fullname template.
  ## @param data.serviceAccount.automountServiceAccountToken Automount service account token for the server service account
  ## @param data.serviceAccount.annotations Annotations for service account. Evaluated as a template. Only used if `create` is `true`.
  ##
  serviceAccount:
    create: true
    name: ""
    automountServiceAccountToken: false
    annotations: {}

  ## Enable HorizontalPodAutoscaler for Opensearch data pods
  ## ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/
  ## @param data.autoscaling.enabled Whether enable horizontal pod autoscale
  ## @param data.autoscaling.minReplicas Configure a minimum amount of pods
  ## @param data.autoscaling.maxReplicas Configure a maximum amount of pods
  ## @param data.autoscaling.targetCPU Define the CPU target to trigger the scaling actions (utilization percentage)
  ## @param data.autoscaling.targetMemory Define the memory target to trigger the scaling actions (utilization percentage)
  ##
  autoscaling:
    enabled: false
    minReplicas: 3
    maxReplicas: 11
    targetCPU: ""
    targetMemory: ""

## @section Ingest parameters

## Opensearch ingest node parameters
##
ingest:
  ## @param ingest.enabled Enable ingest nodes
  ##
  enabled: true
  ## @param ingest.fullnameOverride String to fully override opensearch.ingest.fullname template with a string
  ##
  fullnameOverride: ""
  ## @param ingest.replicaCount Desired number of Opensearch ingest nodes
  ##
  replicaCount: 2
  ## Update strategy for Opensearch ingest statefulset
  ## ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#update-strategies
  ## @param ingest.updateStrategy.type Update strategy for Ingest statefulset
  ##
  updateStrategy:
    type: RollingUpdate
  ## @param ingest.heapSize Ingest node heap size
  ##
  heapSize: 128m
  ## @param ingest.podAnnotations Annotations for ingest pods.
  ##
  podAnnotations: {}
  ## @param ingest.hostAliases Add deployment host aliases
  ## https://kubernetes.io/docs/concepts/services-networking/add-entries-to-pod-etc-hosts-with-host-aliases/
  ##
  hostAliases: []
  ## @param ingest.schedulerName Name of the k8s scheduler (other than default)
  ## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/
  ##
  schedulerName: ""
  ## @param ingest.podLabels Extra labels to add to Pod
  ##
  podLabels: {}
  ## Pod Security Context for ingest pods.
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  ## @param ingest.securityContext.enabled Enable security context for ingest pods
  ## @param ingest.securityContext.fsGroup Group ID for the container for ingest pods
  ## @param ingest.securityContext.runAsUser User ID for the container for ingest pods
  ##
  securityContext:
    enabled: true
    fsGroup: 1000
    runAsUser: 1000
  ## Pod Security Context for ingest pods.
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  ## @param ingest.podSecurityContext.enabled Enable security context for ingest pods
  ## @param ingest.podSecurityContext.fsGroup Group ID for the container for ingest pods
  ##
  podSecurityContext:
    enabled: true
    fsGroup: 1000
  ## Container Security Context for the main container.
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  ## @param ingest.containerSecurityContext.enabled Enable security context for the main container
  ## @param ingest.containerSecurityContext.runAsUser User ID for the container for the main container
  ## @param ingest.containerSecurityContext.runAsNonRoot Indicates that the container must run as a non-root user
  ## @param ingest.containerSecurityContext.allowPrivilegeEscalation Controls whether a process can gain more privileges than its parent process
  ##
  containerSecurityContext:
    enabled: true
    runAsUser: 1000
    runAsNonRoot: true
    allowPrivilegeEscalation: false
  ## @param ingest.podAffinityPreset Ingest Pod affinity preset. Ignored if `affinity` is set. Allowed values: `soft` or `hard`
  ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
  ##
  podAffinityPreset: ""
  ## @param ingest.podAntiAffinityPreset Ingest Pod anti-affinity preset. Ignored if `affinity` is set. Allowed values: `soft` or `hard`
  ## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
  ##
  podAntiAffinityPreset: ""
  ## Node affinity preset
  ## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity
  ## Allowed values: soft, hard
  ## @param ingest.nodeAffinityPreset.type Ingest Node affinity preset type. Ignored if `affinity` is set. Allowed values: `soft` or `hard`
  ## @param ingest.nodeAffinityPreset.key Ingest Node label key to match Ignored if `affinity` is set.
  ## @param ingest.nodeAffinityPreset.values Ingest Node label values to match. Ignored if `affinity` is set.
  ##
  nodeAffinityPreset:
    type: ""
    ## E.g.
    ## key: "kubernetes.io/e2e-az-name"
    ##
    key: ""
    ## E.g.
    ## values:
    ##   - e2e-az1
    ##   - e2e-az2
    ##
    values: []
  ## @param ingest.affinity Ingest Affinity for pod assignment
  ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
  ## Note: podAffinityPreset, podAntiAffinityPreset, and  nodeAffinityPreset will be ignored when it's set
  ##
  affinity: {}
  ## @param ingest.priorityClassName Ingest pods Priority Class Name
  ## ref: https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/#priorityclass
  ##
  priorityClassName: ""
  ## @param ingest.nodeSelector Ingest Node labels for pod assignment
  ## Ref: https://kubernetes.io/docs/user-guide/node-selection/
  ##
  nodeSelector: {}
  ## @param ingest.tolerations Ingest Tolerations for pod assignment
  ## Ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
  ##
  tolerations: []
  ## @param ingest.topologySpreadConstraints Topology Spread Constraints for pod assignment spread across your cluster among failure-domains. Evaluated as a template
  ## Ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/#spread-constraints-for-pods
  ##
  topologySpreadConstraints: []
  ## Opensearch ingest container's resource requests and limits
  ## ref: https://kubernetes.io/docs/user-guide/compute-resources/
  ## We usually recommend not to specify default resources and to leave this as a conscious
  ## choice for the user. This also increases chances charts run on environments with little
  ## resources, such as Minikube. If you do want to specify resources, uncomment the following
  ## lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  ## @param ingest.resources.limits The resources limits for the container
  ## @param ingest.resources.requests [object] The requested resources for the container
  ##
  resources:
    ## Example:
    ## limits:
    ##    cpu: 100m
    ##    memory: 384Mi
    limits: {}
    requests:
      cpu: 25m
      memory: 256Mi
  ## Opensearch ingest container's startup probe
  ## ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#container-probes
  ## @param ingest.startupProbe.enabled Enable/disable the startup probe (ingest nodes pod)
  ## @param ingest.startupProbe.initialDelaySeconds Delay before startup probe is initiated (ingest nodes pod)
  ## @param ingest.startupProbe.periodSeconds How often to perform the probe (ingest nodes pod)
  ## @param ingest.startupProbe.timeoutSeconds When the probe times out (ingest nodes pod)
  ## @param ingest.startupProbe.failureThreshold Minimum consecutive failures for the probe to be considered failed after having succeeded
  ## @param ingest.startupProbe.successThreshold Minimum consecutive successes for the probe to be considered successful after having failed (ingest nodes pod)
  ##
  startupProbe:
    enabled: false
    initialDelaySeconds: 90
    periodSeconds: 10
    timeoutSeconds: 5
    successThreshold: 1
    failureThreshold: 5
  ## Opensearch ingest container's liveness probe
  ## ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#container-probes
  ## @param ingest.livenessProbe.enabled Enable/disable the liveness probe (ingest nodes pod)
  ## @param ingest.livenessProbe.initialDelaySeconds Delay before liveness probe is initiated (ingest nodes pod)
  ## @param ingest.livenessProbe.periodSeconds How often to perform the probe (ingest nodes pod)
  ## @param ingest.livenessProbe.timeoutSeconds When the probe times out (ingest nodes pod)
  ## @param ingest.livenessProbe.failureThreshold Minimum consecutive failures for the probe to be considered failed after having succeeded
  ## @param ingest.livenessProbe.successThreshold Minimum consecutive successes for the probe to be considered successful after having failed (ingest nodes pod)
  ##
  livenessProbe:
    enabled: false
    initialDelaySeconds: 90
    periodSeconds: 10
    timeoutSeconds: 5
    successThreshold: 1
    failureThreshold: 5
  ## Opensearch ingest container's readiness probe
  ## ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#container-probes
  ## @param ingest.readinessProbe.enabled Enable/disable the readiness probe (ingest nodes pod)
  ## @param ingest.readinessProbe.initialDelaySeconds Delay before readiness probe is initiated (ingest nodes pod)
  ## @param ingest.readinessProbe.periodSeconds How often to perform the probe (ingest nodes pod)
  ## @param ingest.readinessProbe.timeoutSeconds When the probe times out (ingest nodes pod)
  ## @param ingest.readinessProbe.failureThreshold Minimum consecutive failures for the probe to be considered failed after having succeeded
  ## @param ingest.readinessProbe.successThreshold Minimum consecutive successes for the probe to be considered successful after having failed (ingest nodes pod)
  ##
  readinessProbe:
    enabled: true
    initialDelaySeconds: 30
    periodSeconds: 10
    timeoutSeconds: 5
    successThreshold: 1
    failureThreshold: 5
  ## @param ingest.customStartupProbe Override default startup probe
  ##
  customStartupProbe: {}
  ## @param ingest.customLivenessProbe Override default liveness probe
  ##
  customLivenessProbe: {}
  ## @param ingest.customReadinessProbe Override default readiness probe
  ##
  customReadinessProbe: {}
  ## @param ingest.initContainers Extra init containers to add to the Opensearch ingest pod(s)
  ##
  initContainers: []
  ## @param ingest.sidecars Extra sidecar containers to add to the Opensearch ingest pod(s)
  ##
  sidecars: []
  ## Service parameters for ingest node(s)
  ##
  service:
    ## @param ingest.service.type Kubernetes Service type (ingest nodes)
    ##
    type: LoadBalancer
    ## @param ingest.service.ports.restAPI Opensearch service REST API port
    ## @param ingest.service.ports.transport Opensearch service transport port
    ##
    ports:
      restAPI: 9200
      transport: 9300
    ## Node ports to expose
    ## @param ingest.service.nodePorts.restAPI Node port for REST API
    ## @param ingest.service.nodePorts.transport Node port for REST API
    ## NOTE: choose port between <30000-32767>
    ##
    nodePorts:
      restAPI: ""
      transport: ""
    ## @param ingest.service.annotations Annotations for ingest nodes service
    ## set the LoadBalancer service type to internal only.
    ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#internal-load-balancer
    ##
    annotations: {}
    ## @param ingest.service.loadBalancerIP loadBalancerIP if ingest nodes service type is `LoadBalancer`
    ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#internal-load-balancer
    ##
    loadBalancerIP: ""
    ## @param ingest.service.ipFamilyPolicy ipFamilyPolicy for cluster_manager nodes service
    ## ref: https://kubernetes.io/docs/concepts/services-networking/dual-stack/#services
    ##
    ipFamilyPolicy: PreferDualStack

  ## Pods Service Account
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  ## @param ingest.serviceAccount.create Specifies whether a ServiceAccount should be created
  ## @param ingest.serviceAccount.name Name of the service account to use. If not set and create is true, a name is generated using the fullname template.
  ## @param ingest.serviceAccount.automountServiceAccountToken Automount service account token for the server service account
  ## @param ingest.serviceAccount.annotations Annotations for service account. Evaluated as a template. Only used if `create` is `true`.
  ##
  serviceAccount:
    create: true
    name: ""
    automountServiceAccountToken: false
    annotations: {}
  ## Enable HorizontalPodAutoscaler for Opensearch ingest-only pods
  ## ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/
  ## @param ingest.autoscaling.enabled Whether enable horizontal pod autoscale
  ## @param ingest.autoscaling.minReplicas Configure a minimum amount of pods
  ## @param ingest.autoscaling.maxReplicas Configure a maximum amount of pods
  ## @param ingest.autoscaling.targetCPU Define the CPU target to trigger the scaling actions (utilization percentage)
  ## @param ingest.autoscaling.targetMemory Define the memory target to trigger the scaling actions (utilization percentage)
  ##
  autoscaling:
    enabled: false
    minReplicas: 3
    maxReplicas: 11
    targetCPU: ""
    targetMemory: ""

## @section Metrics parameters

## Opensearch Prometheus plugin exporter configuration
## ref: https://github.com/aiven/prometheus-exporter-plugin-for-opensearch
##
metrics:
  ## @param metrics.enabled Enable prometheus exporter
  ##
  enabled: false
  ## Prometheus Operator ServiceMonitor configuration
  ##
  serviceMonitor:
    ## @param metrics.serviceMonitor.enabled Create ServiceMonitor Resource for scraping metrics using PrometheusOperator
    ##
    enabled: false
    ## @param metrics.serviceMonitor.namespace Namespace which Prometheus is running in
    ## e.g:
    ## namespace: monitoring
    ##
    namespace: ""
    ## @param metrics.serviceMonitor.jobLabel The name of the label on the target service to use as the job name in prometheus.
    ##
    jobLabel: ""
    ## @param metrics.serviceMonitor.interval Interval at which metrics should be scraped
    ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#endpoint
    ##
    interval: ""
    ## @param metrics.serviceMonitor.scrapeTimeout Timeout after which the scrape is ended
    ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#endpoint
    ##
    scrapeTimeout: ""
    ## @param metrics.serviceMonitor.relabelings RelabelConfigs to apply to samples before scraping
    ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#relabelconfig
    ##
    relabelings: []
    ## @param metrics.serviceMonitor.metricRelabelings MetricRelabelConfigs to apply to samples before ingestion
    ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#relabelconfig
    ##
    metricRelabelings: []
    ## @param metrics.serviceMonitor.selector ServiceMonitor selector labels
    ## ref: https://github.com/bitnami/charts/tree/master/bitnami/prometheus-operator#prometheus-configuration
    ##
    ## selector:
    ##   prometheus: my-prometheus
    ##
    selector: {}
    ## @param metrics.serviceMonitor.labels Extra labels for the ServiceMonitor
    ##
    labels: {}
    ## @param metrics.serviceMonitor.honorLabels honorLabels chooses the metric's labels on collisions with target labels
    ##
    honorLabels: false
  ## Prometheus Operator PrometheusRule configuration
  ##
  prometheusRule:
    ## @param metrics.prometheusRule.enabled Creates a Prometheus Operator PrometheusRule (also requires `metrics.enabled` to be `true` and `metrics.prometheusRule.rules`)
    ##
    enabled: false
    ## @param metrics.prometheusRule.namespace Namespace for the PrometheusRule Resource (defaults to the Release Namespace)
    ##
    namespace: ""
    ## @param metrics.prometheusRule.additionalLabels Additional labels that can be used so PrometheusRule will be discovered by Prometheus
    ##
    additionalLabels: {}
    ## @param metrics.prometheusRule.rules Prometheus Rule definitions
      # - alert: es cluster error
      #   annotations:
      #     summary: "es cluster error"
      #     description: "es cluster error, cluster state {{`{{`}} $labels.color {{`}}`}}"
      #   expr: opensearch_cluster_health_status{color="red"} ==1 or opensearch_cluster_health_status{color="yellow"} ==1
      #   for: 1m
      #   labels:
      #     severity: critical
      #     group: PaaS

      # - alert: Opensearch_UP
      #   expr: opensearch_up{job="opensearch"} != 1
      #   for: 120s
      #   labels:
      #     - severity: "alert"
      #     - value: "{{`{{`}} $value {{`}}`}}"
      #   annotations:
      #     summary = "Instance {{ $labels.instance }}: opensearch instance status is not 1",
      #     description = "This server's opensearch instance status has a value of {{ $value }}.",

      # - alert: Opensearch_Cluster_Health_RED
      #   expr: opensearch_cluster_health_status{color="red"}==1
      #   for: 300s
      #   labels:
      #     - severity: "alert"
      #     - value: "{{`{{`}} $value {{`}}`}}"
      #   annotations:
      #     summary = "Instance {{ $labels.instance }}: not all primary and replica shards are allocated in opensearch cluster {{ $labels.cluster }}",
      #     description = "Instance {{ $labels.instance }}: not all primary and replica shards are allocated in opensearch cluster {{ $labels.cluster }}.",

      # - alert: Opensearch_Cluster_Health_Yellow
      #   expr: opensearch_cluster_health_status{color="yellow"}==1
      #   for: 300s
      #   labels:
      #     - severity: "alert"
      #     - value: "{{`{{`}} $value {{`}}`}}"
      #   annotations:
      #     summary = "Instance {{ $labels.instance }}: not all primary and replica shards are allocated in opensearch cluster {{ $labels.cluster }}",
      #     description = "Instance {{ $labels.instance }}: not all primary and replica shards are allocated in opensearch cluster {{ $labels.cluster }}.",

      # - alert: Opensearch_JVM_Heap_Too_High
      #   expr: opensearch_jvm_memory_used_bytes{area="heap"} / opensearch_jvm_memory_max_bytes{area="heap"} > 0.8
      #   for: 15m
      #   labels:
      #     - severity: "alert"
      #     - value: "{{`{{`}} $value {{`}}`}}"
      #   annotations:
      #     summary = "Opensearch node {{ $labels.instance }} heap usage is high",
      #     description = "The heap in {{ $labels.instance }} is over 80% for 15m.",

      # - alert: Opensearch_health_up
      #   expr: opensearch_cluster_health_up !=1
      #   for: 1m
      #   labels:
      #     - severity: "alert"
      #     - value: "{{`{{`}} $value {{`}}`}}"
      #   annotations:
      #     summary = "Opensearch node: {{ $labels.instance }} last scrape of the Opensearch cluster health failed",
      #     description = "Opensearch node: {{ $labels.instance }} last scrape of the Opensearch cluster health failed",

      # - alert: Opensearch_Too_Few_Nodes_Running
      #   expr: opensearch_cluster_health_number_of_nodes < 3
      #   for: 5m
      #   labels:
      #     - severity: "alert"
      #     - value: "{{`{{`}} $value {{`}}`}}"
      #   annotations:
      #     description="There are only {{$value}} < 3 Opensearch nodes running",
      #     summary="Opensearch running on less than 3 nodes"

      # - alert: Opensearch_Count_of_JVM_GC_Runs
      #   expr: rate(opensearch_jvm_gc_collection_seconds_count{}[5m])>5
      #   for: 60s
      #   labels:
      #     - severity: "warning"
      #     - value: "{{`{{`}} $value {{`}}`}}"
      #   annotations:
      #     summary = "Opensearch node {{ $labels.instance }}: Count of JVM GC runs > 5 per sec and has a value of {{ $value }}",
      #     description = "Opensearch node {{ $labels.instance }}: Count of JVM GC runs > 5 per sec and has a value of {{ $value }}",

      # - alert: Opensearch_GC_Run_Time
      #   expr: rate(opensearch_jvm_gc_collection_seconds_sum[5m])>0.3
      #   for: 60s
      #   labels:
      #     - severity: "warning"
      #     - value: "{{`{{`}} $value {{`}}`}}"
      #   annotations:
      #     summary = "Opensearch node {{ $labels.instance }}: GC run time in seconds > 0.3 sec and has a value of {{ $value }}",
      #     description = "Opensearch node {{ $labels.instance }}: GC run time in seconds > 0.3 sec and has a value of {{ $value }}",

      # - alert: Opensearch_json_parse_failures
      #   expr: opensearch_cluster_health_json_parse_failures>0
      #   for: 60s
      #   labels:
      #     - severity: "warning"
      #     - value: "{{`{{`}} $value {{`}}`}}"
      #   annotations:
      #     summary = "Opensearch node {{ $labels.instance }}: json parse failures > 0 and has a value of {{ $value }}",
      #     description = "Opensearch node {{ $labels.instance }}: json parse failures > 0 and has a value of {{ $value }}",


      # - alert: Opensearch_breakers_tripped
      #   expr: rate(opensearch_breakers_tripped{}[5m])>0
      #   for: 60s
      #   labels:
      #     - severity: "warning"
      #     - value: "{{`{{`}} $value {{`}}`}}"
      #   annotations:
      #     summary = "Opensearch node {{ $labels.instance }}: breakers tripped > 0 and has a value of {{ $value }}",
      #     description = "Opensearch node {{ $labels.instance }}: breakers tripped > 0 and has a value of {{ $value }}",

      # - alert: Opensearch_health_timed_out
      #   expr: opensearch_cluster_health_timed_out>0
      #   for: 60s
      #   labels:
      #     - severity: "warning"
      #     - value: "{{`{{`}} $value {{`}}`}}"
      #   annotations:
      #     summary = "Opensearch node {{ $labels.instance }}: Number of cluster health checks timed out > 0 and has a value of {{ $value }}",
      #     description = "Opensearch node {{ $labels.instance }}: Number of cluster health checks timed out > 0 and has a value of {{ $value }}",
    ##
    rules: []

## @section Sysctl Image parameters

## Kernel settings modifier image
##
sysctlImage:
  ## @param sysctlImage.enabled Enable kernel settings modifier image
  ##
  enabled: true
  ## @param sysctlImage.registry Kernel settings modifier image registry
  ## @param sysctlImage.repository Kernel settings modifier image repository
  ## @param sysctlImage.tag Kernel settings modifier image tag
  ## @param sysctlImage.pullPolicy Kernel settings modifier image pull policy
  ## @param sysctlImage.pullSecrets Kernel settings modifier image pull secrets
  ##
  registry: docker.io
  repository: bitnami/os-shell
  tag: 12-debian-12-r43
  ## Specify a imagePullPolicy
  ## Defaults to 'Always' if image tag is 'latest', else set to 'IfNotPresent'
  ## ref: https://kubernetes.io/docs/user-guide/images/#pre-pulling-images
  ##
  pullPolicy: IfNotPresent
  ## Optionally specify an array of imagePullSecrets.
  ## Secrets must be manually created in the namespace.
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
  ## e.g:
  ## pullSecrets:
  ##   - myRegistryKeySecretName
  ##
  pullSecrets: []
  ## Init container' resource requests and limits
  ## ref: https://kubernetes.io/docs/user-guide/compute-resources/
  ## We usually recommend not to specify default resources and to leave this as a conscious
  ## choice for the user. This also increases chances charts run on environments with little
  ## resources, such as Minikube. If you do want to specify resources, uncomment the following
  ## lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  ## @param sysctlImage.resources.limits The resources limits for the container
  ## @param sysctlImage.resources.requests The requested resources for the container
  ##
  resources:
    ## Example:
    ## limits:
    ##    cpu: 100m
    ##    memory: 128Mi
    limits: {}
    ## Examples:
    ## requests:
    ##    cpu: 100m
    ##    memory: 128Mi
    requests: {}

## @section VolumePermissions parameters

## Init containers parameters:
## volumePermissions: Change the owner and group of the persistent volume mountpoint to runAsUser:fsGroup values from the securityContext section.
##
volumePermissions:
  ## @param volumePermissions.enabled Enable init container that changes volume permissions in the data directory (for cases where the default k8s `runAsUser` and `fsUser` values do not work)
  ##
  enabled: false
  ## @param volumePermissions.image.registry Init container volume-permissions image registry
  ## @param volumePermissions.image.repository Init container volume-permissions image name
  ## @param volumePermissions.image.tag Init container volume-permissions image tag
  ## @param volumePermissions.image.pullPolicy Init container volume-permissions image pull policy
  ## @param volumePermissions.image.pullSecrets Init container volume-permissions image pull secrets
  ##
  image:
    registry: docker.io
    repository: bitnami/bitnami-shell
    tag: 10-debian-10-r328
    pullPolicy: IfNotPresent
    ## Optionally specify an array of imagePullSecrets.
    ## Secrets must be manually created in the namespace.
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
    ## e.g:
    ## pullSecrets:
    ##   - myRegistryKeySecretName
    ##
    pullSecrets: []
  ## Init container' resource requests and limits
  ## ref: https://kubernetes.io/docs/user-guide/compute-resources/
  ## We usually recommend not to specify default resources and to leave this as a conscious
  ## choice for the user. This also increases chances charts run on environments with little
  ## resources, such as Minikube. If you do want to specify resources, uncomment the following
  ## lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  ## @param volumePermissions.resources.limits The resources limits for the container
  ## @param volumePermissions.resources.requests The requested resources for the container
  ##
  resources:
    ## Example:
    ## limits:
    ##    cpu: 100m
    ##    memory: 128Mi
    limits: {}
    ## Examples:
    ## requests:
    ##    cpu: 100m
    ##    memory: 128Mi
    requests: {}

## @section Exposure parameters
##

## Configure the ingress resource that allows you to access the
## Dashboard installation. Set up the URL
## ref: https://kubernetes.io/docs/user-guide/ingress/
##
ingress:
  ## @param ingress.enabled Enable ingress controller resource
  ##
  enabled: false
  ## @param ingress.pathType Ingress Path type
  ##
  pathType: ImplementationSpecific
  ## @param ingress.apiVersion Override API Version (automatically detected if not set)
  ##
  apiVersion: ""
  ## @param ingress.hostname Default host for the ingress resource. If specified as "*" no host rule is configured
  ##
  hostname: opensearch.local
  ## @param ingress.path The Path to Dashboard. You may need to set this to '/*' in order to use this with ALB ingress controllers.
  ##
  path: /
  ## @param ingress.annotations Additional annotations for the Ingress resource. To enable certificate autogeneration, place here your cert-manager annotations.
  ## For a full list of possible ingress annotations, please see
  ## ref: https://github.com/kubernetes/ingress-nginx/blob/master/docs/user-guide/nginx-configuration/annotations.md
  ## Use this parameter to set the required annotations for cert-manager, see
  ## ref: https://cert-manager.io/docs/usage/ingress/#supported-annotations
  ##
  ## e.g:
  ## annotations:
  ##   cert-manager.io/cluster-issuer: cluster-issuer-name
  ##   nginx.ingress.kubernetes.io/proxy-buffer-size: 256k
  ##   nginx.ingress.kubernetes.io/proxy-buffers-number: "4"
  ##
  annotations: {}
  ## @param ingress.tls Enable TLS configuration for the hostname defined at ingress.hostname parameter
  ## TLS certificates will be retrieved from a TLS secret with name: {{- printf "%s-tls" .Values.ingress.hostname }}
  ## You can use the ingress.secrets parameter to create this TLS secret or rely on cert-manager to create it
  ##
  tls: false
  ## @param ingress.selfSigned Create a TLS secret for this ingress record using self-signed certificates generated by Helm
  ##
  selfSigned: false
  ## @param ingress.extraHosts The list of additional hostnames to be covered with this ingress record.
  ## Most likely the hostname above will be enough, but in the event more hosts are needed, this is an array
  ## extraHosts:
  ## - name: opensearch.local
  ##   path: /
  ##
  extraHosts: []
  ## @param ingress.extraPaths Additional arbitrary path/backend objects
  ## For example: The ALB ingress controller requires a special rule for handling SSL redirection.
  ## extraPaths:
  ## - path: /*
  ##   backend:
  ##     serviceName: ssl-redirect
  ##     servicePort: use-annotation
  ##
  extraPaths: []
  ## @param ingress.extraTls The tls configuration for additional hostnames to be covered with this ingress record.
  ## see: https://kubernetes.io/docs/concepts/services-networking/ingress/#tls
  ## extraTls:
  ## - hosts:
  ##     - Dashboard.local
  ##   secretName: Dashboard.local-tls
  ##
  extraTls: []
  ## @param ingress.secrets If you're providing your own certificates, please use this to add the certificates as secrets
  ## key and certificate should start with -----BEGIN CERTIFICATE----- or
  ## -----BEGIN RSA PRIVATE KEY-----
  ##
  ## name should line up with a tlsSecret set further up
  ## If you're using cert-manager, this is unneeded, as it will create the secret for you if it is not set
  ##
  ## It is also possible to create and manage the certificates outside of this helm chart
  ## Please see README.md for more information
  ## e.g:
  ## - name: Dashboard.local-tls
  ##   key:
  ##   certificate:
  ##
  secrets: []
  ## @param ingress.ingressClassName IngressClass that will be be used to implement the Ingress (Kubernetes 1.18+)
  ## This is supported in Kubernetes 1.18+ and required if you have more than one IngressClass marked as the default for your cluster .
  ## ref: https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/
  ##
  ingressClassName: ""
  ## @param ingress.extraRules The list of additional rules to be added to this ingress record. Evaluated as a template
  ## Useful when looking for additional customization, such as using different backend
  ##
  extraRules: []

## @section SecurityAdmin parameters

## Opensearch securityadmin parameters
##
securityadmin:
  ## @param securityadmin.enabled Enable Opensearch SecurityAdmin hook job
  enabled: true
  ## @param securityadmin.schedulerName Name of the k8s scheduler (other than default)
  ## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/
  ##
  schedulerName: ""
  ## @param securityadmin.podAnnotations Annotations to add to the pod
  ##
  podAnnotations: {}
  ## @param securityadmin.podLabels Extra labels to add to Pod
  ##
  podLabels: {}
  ## @param securityadmin.initContainers Extra init containers to add to the Opensearch coordinating pod(s)
  ##
  initContainers: []
  ## @param securityadmin.sidecars Extra sidecar containers to add to the Opensearch ingest pod(s)
  ##
  sidecars: []
  ## @param securityadmin.affinity SecurityAdmin Affinity for pod assignment
  ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
  ## Note: podAffinityPreset, podAntiAffinityPreset, and  nodeAffinityPreset will be ignored when it's set
  ##
  affinity: {}
  ## @param securityadmin.nodeSelector SecurityAdmin Node labels for pod assignment
  ## Ref: https://kubernetes.io/docs/user-guide/node-selection/
  ##
  nodeSelector: {}
  ## @param securityadmin.tolerations SecurityAdmin Tolerations for pod assignment
  ## Ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
  ##
  tolerations: []

  ## Pods Service Account
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  ## @param securityadmin.serviceAccount.create Specifies whether a ServiceAccount should be created
  ## @param securityadmin.serviceAccount.name Name of the service account to use. If not set and create is true, a name is generated using the fullname template.
  ## @param securityadmin.serviceAccount.automountServiceAccountToken Automount service account token for the server service account
  ## @param securityadmin.serviceAccount.annotations Annotations for service account. Evaluated as a template. Only used if `create` is `true`.
  ##
  serviceAccount:
    create: true
    name: ""
    automountServiceAccountToken: false
    annotations: {}

  ## Pod Security Context for securityadmin pods.
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  ## @param securityadmin.securityContext.enabled Enable security context for securityadmin pods
  ## @param securityadmin.securityContext.fsGroup Group ID for the container for securityadmin pods
  ## @param securityadmin.securityContext.runAsUser User ID for the container for securityadmin pods
  ##
  securityContext:
    enabled: true
    fsGroup: 1000
    runAsUser: 1000
  ## Pod Security Context for securityadmin pods.
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  ## @param securityadmin.podSecurityContext.enabled Enable security context for securityadmin pods
  ## @param securityadmin.podSecurityContext.fsGroup Group ID for the container for securityadmin pods
  ##
  podSecurityContext:
    enabled: false
    fsGroup: 1000
  ## Container Security Context for the main container.
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  ## @param securityadmin.containerSecurityContext.enabled Enable security context for the main container
  ## @param securityadmin.containerSecurityContext.runAsUser User ID for the container for the main container
  ## @param securityadmin.containerSecurityContext.runAsNonRoot Indicates that the container must run as a non-root user
  ## @param securityadmin.containerSecurityContext.allowPrivilegeEscalation Controls whether a process can gain more privileges than its parent process
  ##
  containerSecurityContext:
    enabled: true
    runAsUser: 1000
    runAsNonRoot: true
    allowPrivilegeEscalation: false
  ## SecurityAdmin resources requests and limits
  ## ref: https://kubernetes.io/docs/user-guide/compute-resources/
  ## We usually recommend not to specify default resources and to leave this as a conscious
  ## choice for the user. This also increases chances charts run on environments with little
  ## resources, such as Minikube. If you do want to specify resources, uncomment the following
  ## lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  ## @param securityadmin.resources.limits The resources limits for the container
  ## @param securityadmin.resources.requests The requested resources for the container
  ##
  resources:
    ## Example:
    ## limits:
    ##    cpu: 100m
    ##    memory: 128Mi
    limits: {}
    ## Examples:
    ## requests:
    ##    cpu: 100m
    ##    memory: 128Mi
    requests: {}
  ## @param securityadmin.priorityClassName SecurityAdmin Pods Priority Class Name
  ##
  priorityClassName: ""
  ## @param securityadmin.extraVolumes Extra volumes
  ## Example Use Case: mount ssl certificates when opensearch has tls enabled
  ## extraVolumes:
  ##   - name: os-certs
  ##     secret:
  ##       defaultMode: 420
  ##       secretName: os-certs
  extraVolumes: []
  ## @param securityadmin.extraVolumeMounts Mount extra volume(s)
  ## extraVolumeMounts:
  ##   - name: os-certs
  ##     mountPath: /certs
  ##     readOnly: true
  extraVolumeMounts: []

  ## OpenSearch has its own security plugin for authentication and access control. The plugin provides numerous features to help you secure your cluster.
  ## ref: https://opensearch.org/docs/latest/security/configuration/yaml/
  securityConfig:
    # enabled: true
    # admin:
    #   enabled: true
    #   existingSecret: ""
    ## @param securityadmin.securityConfig.path Base path for security YAML files
    path: "/usr/share/opensearch/plugins/opensearch-security/securityconfig"

    ## @param securityadmin.securityConfig.internal_users This file contains any initial users that you want to add to the security plugin’s internal user database.
    ## ref: https://opensearch.org/docs/latest/security/configuration/yaml/#internal_usersyml
    internal_users: {}
    ## @param securityadmin.securityConfig.allowlist You can use allowlist.yml to add any endpoints and HTTP requests to a list of allowed endpoints and requests. If enabled, all users except the super admin are allowed access to only the specified endpoints and HTTP requests, and all other HTTP requests associated with the endpoint are denied. For example, if GET _cluster/settings is added to the allow list, users cannot submit PUT requests to _cluster/settings to update cluster settings.
    ## ref: https://opensearch.org/docs/latest/security/configuration/yaml/#allowlistyml
    allowlist: {}

    ## ref: https://opensearch.org/docs/latest/security/configuration/configuration/
    config:
      dynamic:
        kibana:
          ## @param securityadmin.securityConfig.config.dynamic.kibana.multitenancy_enabled Enable or disable multi-tenancy
          multitenancy_enabled: true
          ## @param securityadmin.securityConfig.config.dynamic.kibana.server_username Must match the name of the OpenSearch Dashboards server user from opensearch_dashboards.yml
          server_username: dashboards
          ## @param securityadmin.securityConfig.config.dynamic.kibana.index Must match the name of the OpenSearch Dashboards index from opensearch_dashboards.yml
          index: ".opensearch_dashboards"
        authc:
          ## ref: https://opensearch.org/docs/latest/security/configuration/configuration/#http-basic
          basic_internal_auth_domain:
            ## @param securityadmin.securityConfig.config.dynamic.authc.basic_internal_auth_domain.http_enabled 
            http_enabled: true
            ## @param securityadmin.securityConfig.config.dynamic.authc.basic_internal_auth_domain.transport_enabled
            transport_enabled: true
            ## @param securityadmin.securityConfig.config.dynamic.authc.basic_internal_auth_domain.order
            order: 0
            http_authenticator:
              ## @param securityadmin.securityConfig.config.dynamic.authc.basic_internal_auth_domain.http_authenticator.type HTTP basic authentication. No additional configuration is needed.
              type: basic
              ## @param securityadmin.securityConfig.config.dynamic.authc.basic_internal_auth_domain.http_authenticator.challenge In most cases, you set the challenge flag to true. The flag defines the behavior of the security plugin if the Authorization field in the HTTP header is not set.
              challenge: false
            authentication_backend:
              ## @param securityadmin.securityConfig.config.dynamic.authc.basic_internal_auth_domain.authentication_backend.type Use the users and roles defined in internal_users.yml for authentication.
              type: internal
      #     openid_auth_domain:
      #       http_enabled: true
      #       transport_enabled: true
      #       order: 1
      #       http_authenticator:
      #         type: openid
      #         challenge: false
      #         config:
      #           subject_key: preferred_username
      #           roles_key: groups
      #           openid_connect_url: https://keycloak.example.com:8080/auth/realms/master/.well-known/openid-configuration
      #       authentication_backend:
      #         type: noop
          clientcert_auth_domain:
            ## @param securityadmin.securityConfig.config.dynamic.authc.clientcert_auth_domain.http_enabled 
            http_enabled: true
            ## @param securityadmin.securityConfig.config.dynamic.authc.clientcert_auth_domain.transport_enabled
            transport_enabled: true
            ## @param securityadmin.securityConfig.config.dynamic.authc.clientcert_auth_domain.order
            order: 1
            http_authenticator:
              ## @param securityadmin.securityConfig.config.dynamic.authc.clientcert_auth_domain.http_authenticator.type TLS client cert authentication. No additional configuration is needed.
              type: clientcert
              ## @param securityadmin.securityConfig.config.dynamic.authc.clientcert_auth_domain.http_authenticator.config.username_attribute TLS cert username attribute for role matching. If omitted DN becomes username.
              config:
                username_attribute: cn
              ## @param securityadmin.securityConfig.config.dynamic.authc.clientcert_auth_domain.http_authenticator.challenge In most cases, you set the challenge flag to true. The flag defines the behavior of the security plugin if the Authorization field in the HTTP header is not set.
              challenge: false
            ## @param securityadmin.securityConfig.config.dynamic.authc.clientcert_auth_domain.authentication_backend.type
            authentication_backend:
              type: noop

    ## @param securityadmin.securityConfig.roles This file contains any initial roles that you want to add to the security plugin. Aside from some metadata, the default file is empty, because the security plugin has a number of static roles that it adds automatically.
    ## ref: https://opensearch.org/docs/latest/security/configuration/yaml/#rolesyml
    roles: {}
    ## @param securityadmin.securityConfig.roles_mapping This file contains any initial role mappings
    ## ref: https://opensearch.org/docs/latest/security/configuration/yaml/#roles_mappingyml
    roles_mapping: {}
    ## @param securityadmin.securityConfig.action_groups This file contains any initial action groups that you want to add to the security plugin.
    ## ref: https://opensearch.org/docs/latest/security/configuration/yaml/#action_groupsyml
    action_groups: {}
    ## @param securityadmin.securityConfig.tenants You can use this file to specify and add any number of OpenSearch Dashboards tenants to your OpenSearch cluster.
    ## ref: https://opensearch.org/docs/latest/security/configuration/yaml/#tenantsyml
    tenants: {}
    ## @param securityadmin.securityConfig.nodes_dn This file lets you add certificates’ distinguished names (DNs) an allow list to enable communication between any number of nodes and/or clusters.
    ## ref: https://opensearch.org/docs/latest/security/configuration/yaml/#nodes_dnyml
    nodes_dn: {}

## @section S3 Snapshot parameters

## Opensearch S3 Snapshot parameters
##
s3Snapshots:
  ## @param s3Snapshots.enabled Enable Opensearch S3 snapshots
  enabled: false
  ## ref: https://opensearch.org/docs/latest/opensearch/snapshot-restore/#amazon-s3
  ##
  config:
    s3:
      client:
        default:
          ## @param s3Snapshots.config.s3.client.default.access_key S3 Access key
          access_key: ""
          ## @param s3Snapshots.config.s3.client.default.secret_key S3 Secret key
          secret_key: ""
          ## @param s3Snapshots.config.s3.client.default.existingSecret Name of an existing secret resource containing the S3 access and secret key
          existingSecret: ""
          ## @param s3Snapshots.config.s3.client.default.existingSecretAccessKey Name of an existing secret key containing the S3 access key
          existingSecretAccessKey: "access_key"
          ## @param s3Snapshots.config.s3.client.default.existingSecretSecretKey Name of an existing secret key containing the S3 secret key
          existingSecretSecretKey: "secret_key"
          ## @param s3Snapshots.config.s3.client.default.endpoint S3 endpoint
          endpoint: s3.amazonaws.com
          ## @param s3Snapshots.config.s3.client.default.region S3 region
          region: us-east-2
          ## @param s3Snapshots.config.s3.client.default.bucket S3 bucket
          bucket: opensearch
          ## @param s3Snapshots.config.s3.client.default.base_path S3 path in bucket
          base_path: snapshots
          ## @param s3Snapshots.config.s3.client.default.max_retries Number of retries if a request fails
          max_retries: 3
          ## @param s3Snapshots.config.s3.client.default.path_style_access Whether to use the deprecated path-style bucket URLs.
          path_style_access: false
          ## @param s3Snapshots.config.s3.client.default.protocol http or https
          protocol: https
          ## @param s3Snapshots.config.s3.client.default.read_timeout The S3 connection timeout
          read_timeout: 50s
          ## @param s3Snapshots.config.s3.client.default.use_throttle_retries Whether the client should wait a progressively longer amount of time (exponential backoff) between each successive retry
          use_throttle_retries: true

## @section Extra secrets in Opensearch keystore

extraSecretsKeystore:
  ## @param extraSecretsKeystore.existingSecret Name of the existing secret containing the entries to add to the Opensearch keystore
  ##
  existingSecret: ""
  ## @param extraSecretsKeystore.secrets Dict of the K/V entries to add to the Opensearch keystore
  ##
  secrets: {}
    # plugins.alerting.destination.email.<sender_name>.username: foo
    # plugins.alerting.destination.email.<sender_name>.password: bar